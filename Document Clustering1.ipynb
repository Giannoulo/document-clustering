{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering - Giannoulopoulos George\n",
    "### Write a program which will parse, analyse and classify the items of the dataset into N (e.g 10) groups based on their text content\n",
    "\n",
    "In order to use clustering on the documents the features need to be extracted. The algorithm to do that is the TF-IDF Vectorizer. TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the documents. The clustering algorithm to be used is K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install et-xmlfile\n",
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the xml file\n",
    "xml = ET.parse('data.source.rss-feeds.xml')  \n",
    "xml_root = xml.getroot()\n",
    "\n",
    "\n",
    "def strip_html(s):\n",
    "    \"\"\"\n",
    "    (str)->(str)\n",
    "    \n",
    "    Remove any html tags from the text using regex. Also remove the extra whitespaces\n",
    "    and the newline escape sequence characters\n",
    "    \"\"\"\n",
    "    no_html = re.sub(\"(<.*?>)\", \" \",s) # Remove html tags\n",
    "    pretty = re.sub('(\\s+)|(\\n)', \" \", no_html) # Remove extra whitespace and \\n\n",
    "    \n",
    "    return  pretty          \n",
    "\n",
    "\n",
    "doc_list=[]\n",
    "for element in xml_root[0].findall('item'):\n",
    "    try:    \n",
    "        title = element[0].text\n",
    "        text = strip_html(element[2].text)\n",
    "        doc_list.append(title+' '+text)\n",
    "    except(TypeError):\n",
    "    # print('TypeError exception')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NUMBER=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:  nbsp  graph  image  title  amp  countries \n",
      "\n",
      "Cluster 2:  employment  education  european  2016  labour  2017 \n",
      "\n",
      "Cluster 3:  skills  labour  market  skill  cedefop  jobs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tfid_construct(documents):\n",
    "    \"\"\"\n",
    "    (list of str)->(sparse matrix, list of str )\n",
    "    \n",
    "    Construct the document-term matrix\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english').fit(documents) # Learn vocabulary and idf from training set.\n",
    "    features_names = vectorizer.get_feature_names() # Get the feature names\n",
    "    tfid_vector = vectorizer.transform(documents) # Transform documents to document-term matrix.\n",
    "    \n",
    "    return tfid_vector, features_names\n",
    "\n",
    "\n",
    "\n",
    "def kmeans_train(tfid_vector, n=10):\n",
    "    \"\"\"\n",
    "    (sparse matrix, int)->(ndarray, ndarray)\n",
    "    \n",
    "    Train the kmeans model on the train matrix and get the centroids and labels.\n",
    "    \"\"\"\n",
    "    kmeans= KMeans(n_clusters=n).fit(tfid_vector)\n",
    "    kmeans_centroids = kmeans.cluster_centers_\n",
    "    kmeans_labels = kmeans.labels_\n",
    "    \n",
    "    return kmeans_centroids, kmeans_labels\n",
    "\n",
    "\n",
    "\n",
    "def top_words(cent, feat, n=10, word_number=6):\n",
    "    \"\"\"\n",
    "    (ndarray, list of str)->()\n",
    "    \n",
    "    Print the top words for each cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_centroid_indexes = cent.argsort()[:, ::-1]\n",
    "    for i in range(n):\n",
    "        print(\"Cluster %d:\" % (i+1), end=' ')\n",
    "\n",
    "        for j in sorted_centroid_indexes[i, :word_number]:\n",
    "            print(' %s' % feat[j], end=' ')\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "vector, features = tfid_construct(doc_list)\n",
    "centroids, labels = kmeans_train(vector, n=CLUSTER_NUMBER)\n",
    "top_words(centroids, features, n=CLUSTER_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: 84 documents\n",
      "Cluster 2: 1188 documents\n",
      "Cluster 3: 385 documents\n"
     ]
    }
   ],
   "source": [
    "def cluster_size(labels_array,n=10):\n",
    "    \"\"\"\n",
    "    (ndarray)->()\n",
    "    \n",
    "    Print the number of items in each cluster\n",
    "    \"\"\"\n",
    "    # Create the dictionary to hold the items of each cluster\n",
    "    cluster_dict={}\n",
    "    for cl in range(n):\n",
    "        cluster_dict[cl] = 0\n",
    "        \n",
    "    for label in labels:\n",
    "        cluster_dict[label] = cluster_dict.get(label) + 1\n",
    "\n",
    "    for i in range(n):\n",
    "        print('Cluster %d: %d documents' % (i+1, cluster_dict.get(i)))\n",
    "        \n",
    "        \n",
    "cluster_size(labels, n=CLUSTER_NUMBER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
